# Fullstacks_Analytics_Pipelines
**Contexte du depot 1** : 
## üìå Contexte

Le d√©p√¥t 1 de **Fullstacks_Analytics_Pipelines** correspond √† la premi√®re √©tape de mise en place d‚Äôun **pipeline ELT** (Extract ‚Äì Load ‚Äì Transform) automatis√© pour la collecte, le stockage et la pr√©paration de donn√©es destin√©es √† des analyses avanc√©es et √† l‚Äôentra√Ænement de mod√®les de Machine Learning.

L‚Äôobjectif principal de ce d√©p√¥t est de cr√©er une **infrastructure fonctionnelle et reproductible** permettant :
- D‚Äôextraire les donn√©es brutes depuis des sources d√©finies (API, fichiers plats, bases de donn√©es‚Ä¶)
- De charger ces donn√©es dans un entrep√¥t (Data Warehouse / Data Lake)
- D‚Äôeffectuer les transformations n√©cessaires pour les rendre exploitables par les outils d‚Äôanalyse et de mod√©lisation

Cette √©tape est cruciale car elle pose les **fondations techniques** de tout le projet **Fullstacks_Analytics_Pipelines**, garantissant la fiabilit√©, la qualit√© et la disponibilit√© des donn√©es pour les d√©p√¥ts suivants (analyses, visualisations, et pr√©dictions).
---

## üéØ Objectifs du D√©p√¥t 1

1. **Mettre en place l‚Äôenvironnement technique**
   - Initialisation du projet avec la structure de r√©pertoires et les d√©pendances requises
   - Configuration d‚ÄôAirflow pour l‚Äôorchestration des t√¢ches ELT
   - D√©finition des param√®tres de connexion aux diff√©rentes sources et destinations de donn√©es

2. **D√©velopper et ex√©cuter un pipeline ELT complet**
   - **Extraction** : r√©cup√©ration des donn√©es brutes depuis la source
   - **Chargement** : insertion des donn√©es brutes dans la couche de stockage
   - **Transformation** : nettoyage, formatage et enrichissement des donn√©es

3. **Garantir la tra√ßabilit√© et la reproductibilit√©**
   - Suivi des ex√©cutions
   - Journalisation des erreurs
   - Versionnage du code
---
# - Phase 1 : Pipeline ELT + Airflow DAGs

Bienvenue dans la **Phase 1** de **Fullstacks_Analytics_Pipelines**, une solution analytique interactive pour explorer et visualiser les donn√©es taxis et limousines de la ville de New York (TLC) du jeu **Taxi & Limousine Commission(TLC)**. 
Ce partie contient l‚Äôimpl√©mentation compl√®te de la premi√®re √©tape du projet, incluant :
- La construction d‚Äôun **pipeline ELT** automatis√©(DAGs Airflow pour l‚Äôautomatisation) pour l‚Äôingestion, le nettoyage et la transformation des donn√©es.
- L‚Äôorchestration avec **Docker Compose Airflow** pour planifier et superviser les t√¢ches.
- Le d√©veloppement, l‚Äôentra√Ænement et l‚Äô√©valuation d‚Äôun **mod√®le de machine learning** pour la pr√©diction (classification).

Cette phase , une solution backend robuste con√ßue pour transformer l'exp√©rience des donn√©es du jeu **Taxi & Limousine Commission**, constitue le socle technique sur lequel s‚Äôappuie la suite du projet, notamment l‚Äôint√©gration dans une application **Streamlit** pour la pr√©diction.

## Aper√ßu de la Phase 2 : D√©veloppement et Entra√Ænement du Mod√®le

**Objectif** : Explorer les donn√©es TLC via le Pipeline d√©velopp√©e en Phase 1 et pr√©senter des insights exploitables √† travers une application web interactive pour les scientifiques, analystes, et studios.

- **Extraction, transformation et chargement des donn√©es (scripts ELT)** :
  - Processus ELT pour extraire les fichiers parquets depuis le site: https://www.nyc.gov/site/tlc/about/about-tlc.page
  - DAGs Airflow pour automatiser le pipeline.
- **Visualisation Interactive** :
  - Cr√©er des graphiques dynamiques avec **Plotly** pour illustrer les insights.
  - Cr√©e les vues SQL
  - Exporte les Parquet pour le cache
  - D√©velopper une application **Streamlit** avec des tableaux dynamiques et une recherche avanc√©e.

- **D√©veloppement et Entra√Ænement du Mod√®le** :
  - script d‚Äôexploration et de feature engineering..
  - Script d‚Äôentra√Ænement du mod√®le.
  - Sauvegarde du mod√®le (.pkl ou .joblib).
  - √âvaluation du mod√®le (MAE, MSE, RMSE, R¬≤‚Ä¶).
  - Scripts pour charger un mod√®le et faire une pr√©diction.

### üõ† Fonctionnalit√©s principales

- **Pipeline ELT automatis√© avec Airflow** :
  -Extraction des donn√©es TLC depuis CSV ou Parquet.
  -Transformation et nettoyage.
  -Chargement des donn√©es trait√©es dans data/processed/.

- **Exploration et visualisation** :
  -Analyse rapide avec Parquet pour r√©duire le temps de chargement.
  -Heatmaps de corr√©lation et statistiques descriptives.

- **Mod√©lisation ML** :
 -Mod√®les : Linear Regression, RandomForest, KNN, XGBoost, CatBoost.
 -Pr√©traitement automatique : imputation, scaling, encodage.
 -S√©lection du meilleur mod√®le selon R¬≤.
 -Logs d√©taill√©s pour suivre l‚Äôex√©cution et les m√©triques.
 -Export du mod√®le entra√Æn√© pour utilisation dans le D√©p√¥t 2 (Streamlit).

### üì¶ Livrables**
√Ä l‚Äôissue de ce d√©p√¥t 1, les livrables attendus sont :
- **1.Pipeline ELT op√©rationnel**
  -DAG Airflow fonctionnel avec t√¢ches ordonnanc√©es
  -Scripts d‚Äôextraction, chargement et transformation valid√©s
- **2.Base de donn√©es aliment√©e**
  -Donn√©es brutes disponibles dans la couche de stockage
  -Donn√©es transform√©es pr√™tes pour exploitation
- **3.Documentation technique**
  -README d√©taillant l‚Äôarchitecture, les technologies, la proc√©dure d‚Äôinstallation et d‚Äôex√©cution
  -Sch√©ma du pipeline ELT
- **4.Fichiers de configuration**
  -Param√©trage des connexions et variables d‚Äôenvironnement
  -Instructions pour adapter le pipeline √† d‚Äôautres sources/destinations

## Structure du Projet


## Technologies Utilis√©es

### üöÄTechnologies utilis√©es
- **Langage** : Python 3.x
- **Orchestration** : Apache Airflow
- **Stockage** : SQLite  
- **ETL/ELT** : Pandas, SQLAlchemy
- **Gestion des d√©pendances** : pip et requirements.txt
- **Versionnage** : Git / GitHub
- **Docker** : Conteneurisation pour d√©ploiements locaux.
- **Docker compose**
---
## Mise en Place de l‚ÄôEnvironnement

### Pr√©requis
- Python 3.11+
- VSCode (recommand√©)
- Git
- Compte GitHub pour le contr√¥le de version
- Compte Render (pour d√©ploiement cloud, gratuit)

### √âtapes d‚ÄôInstallation

1. **Cloner le R√©pertoire**
   ```bash
   git clone https://github.com/kaderkouadio/Fullstacks_Analytics_Pipelines
   cd Fullstacks_Analytics_Pipelines
   ```

2. **Cr√©er et Activer un Environnement Virtuel**
   ```bash
   python3 -m venv .venv
   .\venv\Scripts\activate  # Windows
   # source .venv/bin/activate  # macOS/Linux
   ```

3. **Ouvrir dans VSCode**
   ```bash
   code .
   ```
   S√©lectionnez l‚Äôinterpr√©teur Python du `.venv` si demand√©.

4. **Installer les D√©pendances**
   Assurez-vous que `requirements.txt` contient :
   ```text
   pandas
   plotly
   streamlit
   requests
   python-dotenv
   ```
   Installez :
   ```bash
   pip install requirements.txt
   ```
   ```
---
## D√©ploiement

### github
1. Poussez `Fullstacks_Analytics_Pipelines` dans un r√©pertoire GitHub.
2. Cr√©ez un service Web sur Render :
   - Langage : `Python 3`
   - Commande de Build : `pip install -r requirements.txt`
   - Instance Type : `Free`
3. Ajoutez les variables d‚Äôenvironnement :

---

### üîç Points cl√©s de l‚Äôarchitecture

- **S√©paration claire des √©tapes du pipeline** :  
  Les DAGs Airflow orchestrent les scripts Python situ√©s dans `scripts/`.
- **Gestion optimis√©e des donn√©es** :  
  Fichiers bruts ‚Üí trait√©s ‚Üí cache Parquet pour optimiser les EDA et la mod√©lisation.
- **Suivi complet par logs** :  
  Chaque √©tape est trac√©e dans `logs/` avec horodatage et niveau de d√©tail.
- **Compatibilit√© ML et d√©ploiement** :  
  Les mod√®les entra√Æn√©s dans `models/` sont pr√™ts √† √™tre int√©gr√©s √† l‚Äôapplication Streamlit du **D√©p√¥t 2**.
---

## üöÄ Pourquoi Ce Projet Se D√©marque
Ce projet illustre ma capacit√© √† mettre en place une cha√Æne de traitement de donn√©es **de bout en bout** allant de l‚Äôextraction √† la mise √† disposition d‚Äôun mod√®le pr√™t pour la pr√©diction :  

- **Expertise Technique** : Ma√Ætrise d‚ÄôAirflow pour l‚Äôorchestration, Pandas pour la transformation de donn√©es, SQL pour la cr√©ation de vues, et scikit-learn / XGBoost / CatBoost pour la mod√©lisation.  
- **Pipeline ELT Professionnel** : Automatisation compl√®te du chargement, nettoyage, transformation et stockage des donn√©es dans une base SQLite optimis√©e.  
- **Mod√©lisation Avanc√©e** : Entra√Ænement, √©valuation et s√©lection du meilleur mod√®le sur un dataset structur√©, pr√™t pour int√©gration dans une application de pr√©diction.  
- **Professionnalisme** : Code structur√©, journalisation (logs) compl√®te, scripts modulaires et reproductibles, livrables exploitables pour d√©ploiement.

---

## üîÆ Am√©liorations Futures
- **Connexion √† un Data Warehouse** : Migrer vers PostgreSQL, BigQuery ou Snowflake pour plus de scalabilit√©.  
- **Automatisation CI/CD** : D√©ploiement automatis√© des DAGs et des mod√®les via GitHub Actions ou GitLab CI.  
- **Surveillance du Mod√®le** : Impl√©menter du monitoring (drift de donn√©es, performances en production).  
- **Optimisation des Performances** : Utilisation de formats de stockage haute performance (ex. : Delta Lake, Parquet partitionn√©) et cache Redis.  
- **Extension des Mod√®les** : Tester des mod√®les de deep learning pour am√©liorer la pr√©cision pr√©dictive.
---

## üì¨ Contact

Pour toute question ou collaboration, n‚Äôh√©sitez pas √† me contacter :

**Kader KOUADIO**  
- **LinkedIn** :üë®‚Äçüíª[Koukou Kader Kouadio](https://www.linkedin.com/in/koukou-kader-kouadio-2a32371a4/)
- **Email** :üìß [kkaderkouadio@gmail.com](mailto:kkaderkouadio@gmail.com)

---